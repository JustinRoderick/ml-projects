# -*- coding: utf-8 -*-
"""data_manipulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/JustinRoderick/ml-projects/blob/main/misc/data_manipulation.ipynb

## INSTRUCTIONS

Every learner should submit his/her own homework solutions. However, you are allowed to discuss the homework with each other– but everyone must submit his/her own solution; you may not copy someone else’s solution.

The homework consists of two parts:
1.	Data manipulation
2.	Exploratory Data Analysis

Follow the prompts in the attached jupyter notebook. Download the data and place it in your working directory, or modify the path to upload it to your notebook. Add markdown cells to your analysis to include your solutions, comments, answers. Add as many cells as you need, for easy readability comment when possible.
Hopefully this homework will help you develop skills, make you understand the flow of an EDA, get you ready for individual work.

Submission: Send in both a ipynb and a pdf file of your work.

Good luck!

# Part1: Cleaning, wrangling data

**Data cleaning focuses on removing inaccurate data from your data set whereas data wrangling focuses on transforming the data's format, typically by converting “raw” data into another format more suitable for use.
This excersize uses the traffic_cameras file. Your task is to follow prompts to change, modify your data. Try your best!**
"""

#read in libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn import preprocessing
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# Reading the CSV file
df = pd.read_csv('/content/drive/MyDrive/traffic_cameras.csv')

# Printing top 5 rows
df.head()

"""### 1. How many rows and columns does your data have?"""

# Use shape function to easily find number of rows and cols
numRow = df.shape[0]
numCol = df.shape[1]
print(numRow)
print(numCol)

"""### 2. What can you tell us about the type of variables we have?"""

# dtypes tells you all non null variable types in the dataset
df.dtypes

"""Most Columns have the data type of object. The second most common type is float64 and only one column has int64.

### 3. Delete only the columns that have all null values, name it df1 (nothing else, but null)
"""

# Dropna is used to delete columns or rows with null values
# axis=1 is used to drop columns and 'all' is used to only drop column with all null values
df1 = df.dropna(axis=1, how="all")

df1.info()

"""### 4. Dropp columns that have (any) null values name it df2"""

# The 'any' is used to drop columns that have at least one null value
df2 = df.dropna(axis=1, how="any")

df2.info()
df2.shape

"""### 5. Rename column names in df2 so they are more usable (name the new dataframe df3) to the followings: cam_id, loc_name, cam_stat, atd_loc_id, loc_type, date, comm_stat, comm_stat_date, screen_addr, id, location"""

# Rename method is used to rename the columns when the original name is passed is
df3 = df2.rename(columns={"Camera ID": "cam_id", "Location Name": "loc_name", "Camera Status": "cam_stat", "ATD Location ID": "atd_loc_id", "Location Type": "loc_type", "Modified Date": "date", "IP Comm Status": "comm_stat", "IP Comm Status Date and Time": "comm_stat_date", "Screenshot Address": "screen_addr", "ID": "id", "Location": "location"})

df3.head

"""### 6. Split "date" column into two  new columns within df3 ('Dates' and 'Time') /modify df3 data/"""

# Splits the date column on the first space using 'n=1'
# Splits it into Dates and Time
df3[['Dates', 'Time']] = df3.date.str.split(' ', n=1, expand = True)

# Drops the previos date column
df3 = df3.drop(columns= ['date'])

df3.head

"""### 7. Split atd_loc into two new columns 'Loc' and 'code' within df3"""

# Splits atd_loc_id into Loc and code at the '-'
df3[['Loc', 'code']] = df3.atd_loc_id.str.split('-', n=1, expand = True)
df3 = df3.drop(columns= ['atd_loc_id'])

df3.head

"""### 8. What are the unique values in loc_type?"""

# Returns an array of all the unique values in a specific column
df3.loc_type.unique()

"""### 9. Replace 'ROADWAY' to '0',  'BUILDING' to '1' in the loc_type column within df3"""

# Finds the ROADWAY and BUILDING values in the loc_type column and replaces them with the appropriate value
df3.loc[df3['loc_type'] == 'ROADWAY', 'loc_type'] = 1
df3.loc[df3['loc_type'] == 'BUILDING', 'loc_type'] = 0

df3.head

# Unique values are now 1 and 0
df3.loc_type.unique()

"""### 10. Split on on '/' the loc_name column into two new variables 'corner1', 'corner2'"""

# Splits the loc_name column into corner1 and corner2 at the '/'
df3[['corner1', 'corner2']] = df3.loc_name.str.split('/', n=1, expand = True)
# Drops the old loc_name column
df3 = df3.drop(columns= ['loc_name'])

df3.head

"""# Part2: Exploratory Data Analysis (EDA)

Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.

Follow the lecture notes for ideas of how to perform EDA on your dataset. For help, here are the steps we talked about:

***Steps in EDA***:
1. Provide descriptions of your sample and features
2. Check for missing data
3. Identify the shape of your data
4. Identify significant correlations
5. Spot/deal with outliers in the dataset

These steps are a guidline. Try different things and share your insights about the dataset.

**Don't forget to add "markdown" cells to include your findings or to explain what you are doing**
"""

# Commented out IPython magic to ensure Python compatibility.
# importing packages
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Reading the CSV file
df_fish = pd.read_csv('/content/drive/MyDrive/Fish.csv')

# Printing top 5 rows
df_fish.head()

# Description of the data
# Prints the shape of the data with is the number of rows and cols
print(df_fish.shape)

# Info specifies the column name and row count as well as the data type
df_fish.info()

# Provides basic information abouit the dataset such as mean, standard deviation, and max value
df_fish.describe()

# Check for missing data

# Checks for and null values in the dataset
print(df_fish.isnull().sum())
print()

# Checking for duplicate values in the dataframe
print(df_fish.drop_duplicates)

# Array of all different/unique species of fish in the dataset
print(df_fish.Species.unique())

# Check if the dataframe is balanced
df_fish.value_counts('Species')

# The dataset is not balanced as there are different number of rows for each species

# Visually check if the data is balanced in the form of a bar graph
# Again, the dataset is not balanced among the species
sns.countplot(x='Species', data= df_fish)

# Compares weights between the different species of fish using a stripplot and a violinplot
# From these plots we can see that the Pike has the greatest variation in weight
# While the Smelt has the least amount of variation in weight
print(sns.stripplot(y ='Weight', x = 'Species', data = df_fish))
print(sns.violinplot(y ='Weight', x = 'Species', data = df_fish))

# Compares the Length1 of the different species using a stripplot
sns.stripplot(y ='Length1', x = 'Species', data = df_fish)

# Shows the corrolation of the data to see how closely they are related
# The numbers are between 1 and -1
df_fish.corr(method='pearson')

# Box plots to show outliers for each characteristic
# The cirles past the whisker on the top and bottom of each box plot are the outlier data point and should be removed
# Each boxplot is grouped together by species
df_fish.boxplot(column = ['Weight'], by = 'Species')
df_fish.boxplot(column = ['Length1'], by = 'Species')
df_fish.boxplot(column = ['Length2'], by = 'Species')
df_fish.boxplot(column = ['Length3'], by = 'Species')
df_fish.boxplot(column = ['Height'], by = 'Species')
df_fish.boxplot(column = ['Width'], by = 'Species')

# Uses np.where to return an array of indexes where the values are above or below a certain threshold

# Detect outliers from Roach weight
outliers = np.where((df_fish['Species']== 'Roach') & ((df_fish['Weight']>272) | (df_fish['Weight']<40)))
print(outliers)
# Remove outliers based on index
for x in outliers:
  df_fish.drop(x, inplace=True)

# Detect outliers from Smelt weight
outliers = np.where((df_fish['Species']== 'Smelt') & ((df_fish['Weight']>14)))
# Remove outliers based on index
for x in outliers:
  df_fish.drop(x, inplace=True)

# Detect outliers from Roach Width
outliers = np.where((df_fish['Species']== 'Roach') & ((df_fish['Width']>4.5) | (df_fish['Width']<2.8)))
# Remove outliers based on index
for x in outliers:
  df_fish.drop(x, inplace=True)

# Detect outliers from Smelt Width
outliers = np.where((df_fish['Species']== 'Smelt') & ((df_fish['Width']>1.5)))
# Remove outliers based on index
for x in outliers:
  df_fish.drop(x, inplace=True)

# Reshow Weight and Width boxplots with the previous outliers now removed
df_fish.boxplot(column = ['Weight'], by = 'Species')
df_fish.boxplot(column = ['Width'], by = 'Species')