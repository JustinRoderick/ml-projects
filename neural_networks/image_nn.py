# -*- coding: utf-8 -*-
"""image_nn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/JustinRoderick/ml-projects/blob/main/neural_networks/image_nn.ipynb

Justin Roderick

CAP 4611

# Homework 5

This homework asks you to create your first Neural Network.

In this Homework, you will be using the MNIST Dataset from Keras

You will write code and discussion into code and text cells in this notebook.

If a code block starts with TODO:, this means that you need to write something there.

There are also markdown blocks with questions. Write the answers to these questions in the specified locations.

Some code had been written for you to guide the project. Don't change the already written code.

## Grading
The points add up to 10. Extensive partial credit will be offered. Thus, make sure that you are at least attempting all problems.

Make sure to comment your code, such that the grader can understand what different components are doing or attempting to do.
"""

import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.models import Sequential
from keras.layers import Flatten, Dropout
from keras.utils import to_categorical

"""# A. Setup (2 points).

In this project we are going to create a Neural Network which will train on the MNIST dataset.

The Dataset contains 60000 Training images and 10000 Testing images and the images are from 10 classes.

You will be loading the MNIST datset in training and testing variables.

Display the first 10 images using Matplotlib

Print the shape of the data points

Normalize the Data in [0,1] range.

Convert the labels into one hot code encoding
"""

#TODO
# Load Data from mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Display the first 10 images
plt.figure(figsize=(10, 1))
for i in range(10):
    plt.subplot(1, 10, i + 1)
    plt.imshow(train_images[i], cmap='gray')
    plt.axis('off')
plt.show()

# Print the shapes
print("Shape of training images:", train_images.shape)
print("Shape of testing images:", test_images.shape)

# Normalize the data
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0

# Create the one hot code encoding for the labels
num_classes = 10
trainy = to_categorical(train_labels, num_classes)
testy = to_categorical(test_labels, num_classes)

"""# B. Creating a Dense Neural Network to train on this data. (4 Points)

In this section you will be creating a Neural Netwok based on the specifications
below. This exercise will help you create the network and help run the code.

Input Layer-> Flatten Layer that convert the image matrix to a vector

Hidden Layer->Dense Layer with 4 units and sigmoid activation

Output Layer-> Dense Layer with 10 units and softmax activation.

Print the Summary of the model at the end
"""

#TODO
# Flatten the images and make reshape to 28, 28
trainx = train_images.reshape(train_images.shape[0], 28, 28)
testx = test_images.reshape(test_images.shape[0], 28, 28)

# Create Model
model = Sequential([
    Flatten(input_shape=(28, 28)),  # Input layer
    Dense(4, activation='sigmoid'),  # Hidden layer with 4 units and sigmoid activation
    Dense(10, activation='softmax')  # Output layer with 10 units and softmax activation
])

# Print model summary
model.summary()

"""# Compile and Run the Model"""

model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=['accuracy'])
history=model.fit(trainx,trainy,batch_size=128, epochs=25, verbose= True,validation_split=0.1)
loss,accuracy=model.evaluate(testx,testy,verbose=False)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['training','validation'],loc='best')
plt.show

print(f'Testloss: {loss:.3}')
print(f'Accuracy: {accuracy:.3}')

"""# Extra Credit (2 Points)
Create a Neural Network that will have more than 95% Training Accuracy
"""

#TODO
# Build a more complex model with more layers
model = Sequential([
    Flatten(input_shape=(28, 28)),               # Input layer
    Dense(128, activation='relu'),               # Dense layer with 128 units and ReLU activation
    Dropout(0.5),                                # Dropout layer to reduce overfitting
    Dense(64, activation='relu'),                # Dense layer with 64 units and ReLU activation
    Dropout(0.5),                                # Dropout layer to reduce overfitting
    Dense(32, activation='relu'),                # Dense layer with 32 units and ReLU activation
    Dense(num_classes, activation='softmax')     # Output layer with 10 units and softmax activation
])

# Print model summary
model.summary()

"""# Compile the Model again"""

model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=['accuracy'])
history=model.fit(trainx,trainy,batch_size=128, epochs=25, verbose= True,validation_split=0.1)
loss,accuracy=model.evaluate(testx,testy,verbose=False)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['training','validation'],loc='best')
plt.show

print(f'Testloss: {loss:.3}')
print(f'Accuracy: {accuracy:.3}')

"""# C: Using Convolutions instead of all Dense Layers to train on Dataset    

# (4 Points)

Using the Dense Layer model, we achieved around 84% Accuracy. Lets see if
Convolution Layers can help improve the model.
Create a Neural Network model which will use Convolutional Layers to Learn
about the data.

Acheive a Training Accuracy of 90%
"""

# New model with 2 convolution layers to improve the model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # First convolutional layer
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),                           # Second convolutional layer
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')                         # Output layer with softmax activation
])

# Print model summary
model.summary()

"""# Compile the Convolutional Neural Netowrk"""

model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=['accuracy'])
history=model.fit(trainx,trainy,batch_size=128, epochs=25, verbose= True,validation_split=0.1)
loss,accuracy=model.evaluate(testx,testy,verbose=False)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['training','validation'],loc='best')
plt.show

print(f'Testloss: {loss:.3}')
print(f'Accuracy: {accuracy:.3}')