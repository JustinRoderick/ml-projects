# -*- coding: utf-8 -*-
"""k-nearest_neighbors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/JustinRoderick/ml-projects/blob/main/regressions/k-nearest_neighbors.ipynb

Justin Roderick
5239523

# Homework 4

This homework asks you to perform various experiments with k-nearest neighbors and related algorithms.

The dataset is the same real estate dataset we previously used from:

https://www.kaggle.com/datasets/mirbektoktogaraev/madrid-real-estate-market

You will write code and discussion into code and text cells in this notebook.

If a code block starts with TODO:, this means that you need to write something there.

There are also markdown blocks with questions. Write the answers to these questions in the specified locations.

Some code had been written for you to guide the project. Don't change the already written code.

## Grading
The points add up to 10. Extensive partial credit will be offered. Thus, make sure that you are at least attempting all problems.

Make sure to comment your code, such that the grader can understand what different components are doing or attempting to do.
"""

import numpy as np
import pandas as pd
import sklearn.linear_model
import sklearn.metrics
import sklearn.neighbors

"""## A. Setup for the single variable part

In this part we are creating training data and test data in a single variable setting. The explanatory variable ``sq_mt_built`` is used to predict the price of the house ``buy_price``.

This part is had been done for you, such that the work does not depend on you importing parts from the previous projects.
"""

df = pd.read_csv("houses_Madrid.csv")
#print(f"The lenght {len(df.index)}")
#print(f"The columns of the database {df.columns}")
dfsel = df[["sq_mt_built", "buy_price"]]
dfselnona = dfsel.dropna()
dfselnona.plot.scatter(x="sq_mt_built", y="buy_price")
df_shuffled = dfselnona.sample(frac=1) # shuffle the rows
x = df_shuffled[["sq_mt_built"]].to_numpy(dtype=np.float64)
y = df_shuffled["buy_price"].to_numpy(dtype=np.float64)
training_data_x = x[:16000]
training_data_y = y[:16000]
test_data_x = x[16000:]
test_data_y = y[16000:]
print(f"Training data is composed of {len(training_data_x)} samples.")
print(f"Test data is composed of {len(test_data_x)} samples.")

print(f"Number of NaN-s in training data x: {np.count_nonzero(np.isnan(training_data_x))}")
print(f"Number of NaN-s in training data y: {np.count_nonzero(np.isnan(training_data_y))}")
print(f"Number of NaN-s in test data x: {np.count_nonzero(np.isnan(test_data_x))}")
print(f"Number of NaN-s in test data y: {np.count_nonzero(np.isnan(test_data_y))}")

"""# B. Establishing an error baseline for linear regression

Here we are going to implement the regresssion using linear regression. We also check the performance of the resulting regressor, and print the error.

This part is had been done for you, such that the work does not depend on you importing parts from the previous projects.

You will need to adapt this for the other models.
"""

#%%timeit
# training the linear regressor
regressor = sklearn.linear_model.LinearRegression().fit(training_data_x, training_data_y)
# We will create the predictions yhat for every x from the training data. We will do this one at a time. This is not an efficient way to do it, but it allows you to write and debug functions that return a scalar number
yhats = []
for x in test_data_x:
    yhat = regressor.predict([x])
    yhats.append(yhat[0])

# Now, print some examples of the quality of the classifier
examples = [45, 67, 170, 189, 207]
for i in examples:
    x = test_data_x[i]
    y = test_data_y[i]
    yhat = regressor.predict([x])[0]
    print(f"House {i} with {x} sqmt was sold for {y} euros, but our system predicted {yhat:.2f}")

# Now calculate the root mean square error on the resulting arrays
error = sklearn.metrics.mean_squared_error(yhats, test_data_y, squared=False)
print(f"The mean square error of the linear regression is {error:.2f} euro")

"""# P1: K-nearest neighbors (5 points)

Implement the k-nearest neighbors algorithm in a function called k_nearest_neighbor_predict.

You do not necessarily need to do anything for training, but if you choose to do something, then return the created data structures from the training function, and pass it to the prediction function.

Experiment with the function you have implemented for at least the k values 1, 3 and 15.
"""

#%%timeit

# Function inplements k nearest neighbor without using sklearn
def k_nearest_neighbor_predict(x, k, training_data_x, training_data_y):
  # Dist array to store all distance from points
  dist = []

  # Loops through all points in training data to find distance from test point x
  for i in range(len(training_data_x)):
    distance = np.linalg.norm(x - training_data_x[i])
    dist.append((distance, training_data_y[i]))

  # Sorts the distances calculated above
  dist.sort()
  # Collects the first k shortest distances
  dist = dist[0:k]

  # Finds average price of houses among the shortest k distances
  Prices = 0
  for j in range(k):
    Prices = Prices + dist[j][1]

  Prices = Prices / k
  return Prices

# 13 seconds
#%%timeit
# Arrays for testing and finding MSE
examples = [45, 67, 170, 189, 207]
kValues = [1, 3, 15]
correct = [125000.0, 139000.0, 170000.0, 130000.0, 1950000.0]
pred = []

# Loops through 3 k values
for j in range(3):
  k = kValues[j]
  print("K values is: ", k)

  # Loops through 5 testing houses
  for i in examples:

    x = test_data_x[i]
    y = test_data_y[i]
    # Calulates predicted values which is the average price found above
    y_pred = k_nearest_neighbor_predict(x, k, training_data_x, training_data_y)

    pred.append(y_pred)
    print("House ", i, " with ", x, " sqmt was sold for ", y, " euros, but our system predicted {:.2f}".format(y_pred))

  # Calculates MSE for each k using correct prices and predicted prices
  mse = sklearn.metrics.mean_squared_error(correct, pred, squared=False)
  print("Mean Squared Error for k =", k, ": {:.2f} euros".format(mse))
  pred = []

"""# Questions:
* Q: Do you find that kNN has a higher accuracy than linear regression?
* A: Overall the kNN algorithm had a higher accuracy than Linear regression and the K value of 3 had the highest accuracy among the three tested K values
* Q: How do you find the performance of the implementation? Is it faster or slower than linear regression?
* A: On my computer the kNN algorithm was slower and took 13 sec compared to 6 with LR

# P2: K nearest neighbors using sklearn (2 points)
Use the KNeighborsRegressor function from sklearn to solve the problem. Print the resulting error and samples, similar to the way in Section B.

Experiment with at least with the k (number of neighbors) values 1, 3 and 15.

Answer the questions in the markdown section.
"""

#%%timeit

# Function implements k nearest neighbor using sklearn
def kNearestNeighborSklearn(k, x, training_data_x, training_data_y):
  knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k)
  # Train the model
  knn.fit(training_data_x, training_data_y)
  x = x.reshape(-1, 1)
  # Predict the price of the house using the model
  price = knn.predict(x)
  return(price)

# Loop through different k's
for j in range(3):
  k = kValues[j]
  print("K value is: ",k)

  for i in examples:
    x = test_data_x[i]
    y = test_data_y[i]

    # Predict price using above function
    y_pred = kNearestNeighborSklearn(k, x, training_data_x, training_data_y)
    # Add to prediction array for MSE
    pred.append(y_pred)
    print("House ", i, " with ", x, " sqmt was sold for ", y, " euros, but our system predicted {:.2f}".format(y_pred[0]))

  # Calculate MSE using predicted prices and correct prices
  mse = sklearn.metrics.mean_squared_error(correct, pred, squared=False)
  print("Mean Squared Error for k =", k, ": {:.2f} euros".format(mse))
  pred = []

"""# Questions:
* Q: Do you find that the sklearn version has a higher accuracy than your implementation?
* A: The sklearn version had a much better accuracy when comparing the best out of the three. When K was 1 the error was only 19886.
* Q: How do you find the performance of the sklearn implementation? Is it faster or slower than your implementation?
* A: The sklearn implementation was faster than my implementaion without using sklearn

# C. Setup for the multi-variable part.

For the next part of the project, we are going to work in a multi-variable setting.

This time, there are 7 explanatory variables: ``sq_mt_built``, ``n_rooms``, ``n_bathrooms``, ``is_renewal_needed``, ``is_new_development`` and ``has_fitted_wardrobes``.

We will first create the training and test data while doing some minimal data cleaning.
"""

df = pd.read_csv("houses_Madrid.csv")
#print(f"The columns of the database {df.columns}")

xfields = ["sq_mt_built", "n_rooms", "n_bathrooms", "has_individual_heating", \
           "is_renewal_needed", "is_new_development", "has_fitted_wardrobes"]
yfield = ["buy_price"]
# print (xfields + yfield)
dfsel = df[xfields + yfield]
dfselnona = dfsel.dropna()
df_shuffled = dfselnona.sample(frac=1) # shuffle the rows
x = df_shuffled[xfields].to_numpy(dtype=np.float64)
y = df_shuffled[yfield].to_numpy(dtype=np.float64)
print(x.shape)
training_data_x = x[:8000]
training_data_y = y[:8000]
test_data_x = x[8000:]
test_data_y = y[8000:]
print(f"Training data is composed of {len(training_data_x)} samples.")
print(f"Test data is composed of {len(test_data_x)} samples.")

"""# D. Creating a linear regression multi-variable baseline.

In this section we make a linear regression predictor for the multi-variable case. We also check the performance of the resulting regressor, and print the error.

This part is had been done for you, such that the work does not depend on you importing parts from the previous projects.

You will need to adapt this for the other models.
"""

# training the linear regressor
regressor = sklearn.linear_model.LinearRegression()
regressor.fit(training_data_x, training_data_y)
# We will create the predictions yhat for every x from the training data. We will do this one at a time. This is not an efficient way to do it, but it allows you to write and debug functions that return a scalar number
yhats = []
for x in test_data_x:
    yhat = regressor.predict([x])
    yhats.append(yhat[0])

# Now, print some examples of the quality of the classifier
examples = [45, 67, 170, 189, 207]
for i in examples:
    x = test_data_x[i]

    y = test_data_y[i]
    yhat = regressor.predict([x])[0][0]
    print(f"House {i} with {x[0]} sqmt was sold for {y} euros, but our system predicted {yhat:.2f}")

# Now calculate the root mean square error on the resulting arrays
error = sklearn.metrics.mean_squared_error(yhats, test_data_y, squared=False)
print(f"The mean square error of the linear regression is {error:.2f} euro")

"""# P3: K nearest neighbors using sklearn (3 points)

Use the KNeighborsRegressor function from sklearn to predict the prices of the house. Print the resulting error and samples, similar to the way in Section D.

* Experiment with at least with the k (number of neighbors) values 1, 3 and 15.
* Experiment with different values for the weights parameter (uniform vs distance)
* Experiment with different values for the metric
    * Euclidean
    * Manhattan
    * Some other Minkowski metric parameter (explain which)
* Answer the questions in the markdown section.
"""

# Implements k nearest neigbor using sklearn with multiple parameters
def kNearestNeighborSklearnMultiple(k, x, training_data_x, training_data_y, experiment):
  if(experiment == 0):
    print("Normal Weight")
    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k)
  elif(experiment == 1):
    print("Uniform weight")
    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k, weights="uniform")
  elif(experiment == 2):
    print("Distance weight")
    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k, weights="distance")
  elif(experiment == 3):
    print("Euclidean")
    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k, metric='euclidean')
  elif(experiment == 4):
    print("Manhattan")
    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k, metric='manhattan')
  else:
    print("Chebyshev")
    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=k, metric='chebyshev')

  # Trains model
  knn.fit(training_data_x, training_data_y)
  x = x.reshape(1, -1)
  # Returns predicted price
  price = knn.predict(x)
  return(price)

# Arrays for testing and calculating MSE
kValues = [1, 3, 15]
examples = [45, 67, 170, 189, 207]
correct = [699000.0, 849000.0, 1450000.0, 1280000.0, 725000.0]
pred = []
experiment = -1

for j in range(5):
  experiment += 1

  for k in kValues:
    print("K value is: ",k)
    for i in examples:

      x = test_data_x[i]
      y = test_data_y[i]
      # Finds predicted price by calling above function
      y_pred = kNearestNeighborSklearnMultiple(k, x, training_data_x, training_data_y, experiment)
      # Adds to predictions array
      pred.append(y_pred[0])
      print("House ", i, " with ", x[0], " sqmt was sold for ", y, " euros, but our system predicted ", y_pred[0])

    # Calculates MSE using correct prices and predicted prices
    mse = sklearn.metrics.mean_squared_error(correct, pred, squared=False)
    print("Mean Squared Error for k =", k, ": {:.2f} euros".format(mse))
    print()
    pred = []
  print()

"""# Questions:
* Q: Do you find that the kNN implementation for multiple input variables performs better that linear regression? Discuss.
* A: The kNN implementation for multiple vriables did not do better than linear regression as the errors were much higher overall. When K was 15 the errors were very similar but when K was 1 the error for kNN was very high.
* Q: Do you find that the kNN implementation for multiple input variables performs better than the version for a single variable? Discuss.
* A: The multiple variable version also did not perform better than the single variable version.
* Q: Discuss how sensitive the kNN implementation is to the setting of the hyperparameters.
* A: The hyperparameters did not change the predicted values much but it was dependent on the k value. For instance, when K was 1 the predicted values did not change much at all but when the k values were 15 there was much more variation between the predictions.

"""