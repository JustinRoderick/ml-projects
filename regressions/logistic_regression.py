# -*- coding: utf-8 -*-
"""logistic_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/JustinRoderick/ml-projects/blob/main/regressions/logistic_regression.ipynb

5239523<div>
Justin Roderick<div>
Feb 28, 2024<div>
CAP 4630

# Logistic Regression Homework

This is the 2nd assignment for CAP 4630 and we will implement logistic regression and apply it to two
different datasets. \
You will use **"Tasks"** and **"Hints"** to finish the work. **(Total 100 Points)** \
You are **not** allowed to use Machine Learning libaries such as Scikit-learn and Keras.

**Task Overview:**
- Logistic Regression

## 1 - Logistic Regression ##
### 1.1 Packages

Import useful packages for scientific computing and data processing.

**Tasks:**
1. Import numpy and rename it to np.
2. Import pandas and rename it to pd.
3. Import the pyplot function in the libraray of matplotlib and rename it to plt.

References:
- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.
- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.

**Attention:**
1. After this renaming, you will use the new name to call functions. For example, **numpy** will become **np** in the following sections.
"""

# Import and rename libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""### 1.2 - Data Preparation ##

Prepare the data for regression task. **(20 Points)**

**Tasks:**
1. Load data for logistic regression.
2. **Generate the SCATTER PLOT of the data**.

**Hints:**
1. The data file is "data_logistic.csv", which are exam scores for students.
2. The data is organized by column: x1 (exam 1 score), x2 (exam 2 score), and label y (pass 1 or fail 0).
3. Please use different colors for postive(label=1) and negative(label=0) data.
4. An example of scatter plots is shown below.

![](https://drive.google.com/uc?export=view&id=1CPv5s4W8SkUMa_sXCIz-NejSnFj-e1IH)
"""

# Add .csv file to dataframe
df = pd.read_csv('data_logistic.csv')

# Change values to arrays to plot
x1 = df['x1'].to_numpy()
x2 = df['x2'].to_numpy()
y = df['label'].to_numpy()

# Plot points and add labels
plt.scatter(x1[y==0], x2[y==0], c='blue', label='negative')
plt.scatter(x1[y==1], x2[y==1], c='red', label='positive')
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Exam Scores')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

"""### 1.3 - Sigmoid function ##


Implement sigmoid function so it can be called by the rest of your program. **(20 Points)**

**Tasks:**
1. Implement the sigmoid function (**def sigmoid(z):**).
2. Test the sigmoid function by function **plotting** with test data (X, Y) where Y = sigmoid(X).

**Hints:**  
1. Given the class material, sigmoid function is defined as:
$g(z) = \frac{1}{1+e^{-z}}$.
2. You may consider X = np.linspace(-5, 5, 1000) to plot the curve.
3. Plot Y against X.
4. An example of plot for validation is shown below:

![](https://drive.google.com/uc?export=view&id=18j5oHdw78uVm2WwHsdIb4hwhpXDxR37S)
"""

# Implement sigmoid fuction here
def sigmoid(z):
  return 1/(1 + np.exp(-z))

# Plot sigmoid function
X = np.linspace(-5, 5, 1000)
Y = sigmoid(X)
plt.xlabel('X')
plt.ylabel('sigmoid(x)')
plt.plot(X, Y)
plt.show()

"""### 1.4 - Cost function and gradient ##

Implement the cross entropy cost function and its gradient for logistic regression. **(30 Points)**

**Tasks:**
1. Implement the "cal_cost" to compute the cost.
2. Implement the "cal_grad" to compute the gradients.
3. Test "cal_cost" and "cal_grad" with initial values and print out the results.

**Hint:**
1. The cross entropy cost function (J(θ)) in logistic regression is shown below. It involves two terms, including ylog(h) and (1-y)log(1-h) where h is the function of x.

![](https://drive.google.com/uc?export=view&id=1xLhlPFI4wekwuA7lFm7ebRVt0XBZk3e7)

2. The gradient of the cost J(θ) is a vector of the same length as θ where the $j$th element (for $j = 0, 1, . . . , n)$ is defined below. You may do a hand calculation to justify the first order derivative with the term above.

![](https://drive.google.com/uc?export=view&id=1xfA0A0xyRv2L5JZIdedAmEZxZ3DwpOCF)

3. When you implement J(θ), please use eps = 1e-15 to prevent possible "divide by 0 exception" in second term. You may think about the reason.
4. You may consider the below templates for two functions:

    def cal_cost(theta, X, y):

        htheta = ...
        term1 = ...  /* matrix_multiplication(log(htheta), y)
        term2 = ...  /* matrix_multiplication(log(1-htheta+eps), (1-y))
        J = - 1 / m * (term1 + term2)
        
        return cost
        
    
    def cal_grad(theta, X, y):
        
        htheta = ...
        term1 = ... /* matrix_multiplication(transpose(X), (htheta - y))  //you may think about why transpose(x)
        grad = 1 / m * term1
    
        return grad
5. It involves matrix multiplication and you may consider the function of np.matmul or np.dot.   
        
6. Initialize the intercept term (constant term) with **ones** and the theta with **zeros**. Test the functions with these initial values. \
    **Expected outputs:**\
    Cost at initial theta : 0.6931471805599445\
    Gradient at inital theta : [-0.1        -10.91242026 -11.73652937]

"""

m = x1.size
eps = 1e-15

# Create matrix for inputs and add 1s for bias
X = df[['x1', 'x2']].values
X = np.c_[np.ones((len(X), 1)), X]

# Theta matrix starts with zeros
theta = np.array([0, 0, 0])
theta = theta.reshape(3,1)
theta = theta.astype('float64')

# Input matrix starts with 1s
x = np.array([1, 1, 1])
x = x.reshape(1,3)
y = y.reshape(80,1)

# Cal Cost function
def calCost(theta, X, Y):
  hTheta = sigmoid((np.matmul(X, theta)))
  # Cost Formula from above
  cost = (-1 / m) * np.sum(Y * np.log(hTheta) + (1 - y) * np.log(1 - hTheta + eps))
  return cost

# Calculates Gradient using pseudocode from above
def calGrad(theta, X, Y):
  hTheta = sigmoid((np.matmul(X, theta)))
  term = np.matmul(X.T, (hTheta - Y))
  grad = 1 / m * (term)
  return grad

print("Cost at initial theta: ", calCost(theta, X, y))
print("Gradient at initial theta:  ", calGrad(theta, X, y))

"""## 1.5 Train parameters with Gradient Descent ##


Train parameters using Gradient Descent. **(15 Points)**

**Tasks:**
1. Calculate best fit theta by Gradient Descent with learning rate of **0.001 (1e-3)** and epoch of **80K**. The initial theta from above blocks is used as initial values.
2. Print out the best theta (the last one is considered as the best here) and its corresponding cost.
3. **Plot the decision boundary**.

**Hints:**
1. You may take gradient descent in homework 1 as an template.
2. Derive the boundary line from **sigmoid(theta[0]+ X1 * theta[1] + X2* theta[2])=0.5**. Think about why we get the line by setting **the activated probability to 0.5**. Also, try to calculate the final relationship between X1 and X2. When sigmoid(X) = 0.5, what is the value of x? Check the generated plot in 1.3.
3. The validation of first 5 epochs (updated theta and cost): \
------Epoch 0------\
Theta: [0.0001     0.01091242 0.01173653]\
Cost: 0.6996118077359638\
------Epoch 1------\
Theta: [-0.0001129   0.00053949  0.00229352]\
Cost: 0.6649331468590681\
------Epoch 2------\
Theta: [-5.93604956e-05  8.33145873e-03  1.07754324e-02]\
Cost: 0.6679914364992459\
------Epoch 3------\
Theta: [-0.0002356   0.0004607   0.00370829]\
Cost: 0.6545873034874964\
------Epoch 4------\
Theta: [-0.00020363  0.00683227  0.01065138]\
Cost: 0.6563302142684528
4. You may take the plots below as an exmample:

![](https://drive.google.com/uc?export=view&id=1xLg9LrIF888gGXj3zRAG9iJLsyAmgPQg)

5. It may take ~1 min to finish running.
"""

# Variables for learning rate and epochs and resets theta with zeros
learningRate = 0.001
epochs = 80000
theta = np.array([0, 0, 0])
theta = theta.reshape(3,1)
theta = theta.astype('float64')

# Gradient Descent function goes through all epochs and changes theta using calGrad function
def gradientDescent(X, y, theta, learningRate, epochs):
  for i in range(epochs):
    theta -= learningRate * calGrad(theta, X, y)
    print("----Epoch ", i, '----')
    print("Theta: ", theta)
    curCost = calCost(theta, X, y)
    print("Cost: ", curCost)
  return (theta, curCost)

# Save final and curCost from gradientDescent and print it out
(theta, curCost) = gradientDescent(X, y, theta, learningRate, epochs)
print("Best fit Theta: ", theta)
print("Final Cost: ", curCost)

# Create scatter plot for inputs
x1 = df['x1'].to_numpy()
x2 = df['x2'].to_numpy()
y = df['label'].to_numpy()
plt.scatter(x1[y==0], x2[y==0], c='blue', label='negative')
plt.scatter(x1[y==1], x2[y==1], c='red', label='positive')

# Use formula from above to find x2Values from the x1 values and optimized theta
x2Values = (-theta[0] - (theta[1] * x1)) / theta[2]

# Plot this line using x1 and x2 values
plt.plot(x1, x2Values, color='green', label='Decision Boundary')
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Exam Scores')
plt.legend()
plt.grid(True)
plt.show()

"""
### 1.6 Evaluating Logistic Regression

Evaluate the model with given data. **(15 Points)**

**Tasks:**
1. Calculate the training accuracy and **PRINT IT OUT**.
2. Evaluate the predicted probability of the learnt model with x1 = 56 and x2 = 32 and **PRINT IT OUT**.


**Hints:**  
1. Positive(prediction>0.5) and negative(prediction<=0.5).
2. The prediction results are based on acceptance probability. Given the two exam scores, we expected the model yields either high probability of "fail" or low probability of "pass".
3. Training accuracy should be around **85%**."""

# Finds predicted probablity values from X inputs and optimized theta
def prediction(X, theta):
  z = np.matmul(X, theta)
  predictions = sigmoid(z)
  return predictions

# Finds how many of the prodicted probabilty from above were correct by comparing with y labels
def trainingAccuracy(X, y, theta):
  predictions = prediction(X, theta)
  positivePredictions = (predictions > 0.5)

  # Every time a prediction was correct it adds 1 to count
  count = 0
  for i in range(m):
    if(((positivePredictions[i] == True) and (y[i] == 1)) or ((positivePredictions[i] == False) and (y[i] == 0))):
      count = count + 1

  # Accuracy is total correct predictions divided by 80
  accuracy = count / m
  accuracy = accuracy * 100
  return accuracy

print("Training Accuracy: ", trainingAccuracy(X, y, theta), "%")

# New inputs
newX = [1, 56, 32]
z = np.matmul(newX, theta)
predictedProbability = sigmoid(z)
print("Predicted Probability for x1 = 56 and x2 = 32: ", predictedProbability[0])